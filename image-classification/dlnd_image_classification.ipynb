{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 8:\n",
      "Image - Min Value: 8 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 8 Name: ship\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJugBoDBBDwCFCXoAKEzQA0Bh/wc5+z+o+88SDwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f20858c14e0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 8\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return x/255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    N=len(x)\n",
    "    z=np.zeros((N,10))\n",
    "    z[np.arange(N),x]=1\n",
    "    return z\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32,[None, image_shape[0], image_shape[1], image_shape[2]],name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32,[None,n_classes],name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    i=x_tensor.get_shape().as_list()\n",
    "    d=[conv_ksize[0], conv_ksize[1], i[3], conv_num_outputs]\n",
    "    W=tf.Variable(tf.truncated_normal(d,stddev=0.08))\n",
    "    x = tf.nn.conv2d(x_tensor, W, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    b=tf.Variable(tf.zeros([conv_num_outputs]))    \n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.nn.max_pool(x, ksize=[1, pool_ksize[0], pool_ksize[1], 1], strides=[1, pool_strides[0], pool_strides[1], 1],padding='SAME')\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "\n",
    "    d=x_tensor.get_shape().as_list()\n",
    "    flat = tf.reshape(x_tensor, [-1, d[1]*d[2]*d[3]])\n",
    "    return flat\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    d=x_tensor.get_shape().as_list()\n",
    "    #print(d)\n",
    "    W=tf.Variable(tf.truncated_normal([d[1],num_outputs],stddev=0.08))\n",
    "    #print(W.shape)\n",
    "    b=tf.Variable(tf.zeros([num_outputs]))\n",
    "    #print(b.shape)\n",
    "    fc=tf.matmul(x_tensor,W)+b\n",
    "    #print(fc.shape)\n",
    "    return tf.nn.relu(fc)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    d=x_tensor.get_shape().as_list()\n",
    "    #print(d)\n",
    "    W=tf.Variable(tf.truncated_normal([d[1],num_outputs],stddev=0.08))\n",
    "    #print(W.shape)\n",
    "    b=tf.Variable(tf.zeros([num_outputs]))\n",
    "    #print(b.shape)\n",
    "    fc=tf.matmul(x_tensor,W)+b\n",
    "    #print(fc.shape)\n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    cn1=conv2d_maxpool(x, 32, (5,5), (1,1), (2,2), (2,2))    \n",
    "    cn1=tf.nn.dropout(cn1,keep_prob)\n",
    "    cn2=conv2d_maxpool(cn1, 64, (5,5), (1,1), (2,2), (2,2))    \n",
    "    cn3=conv2d_maxpool(cn2, 128, (5,5), (1,1), (2,2), (2,2))    \n",
    "    cn3=tf.nn.dropout(cn3,keep_prob)\n",
    "    #cn4=conv2d_maxpool(cn3, 256, (3,3), (1,1), (2,2), (2,2))    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat=flatten(cn3)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc1=fully_conn(flat,512)\n",
    "    fc1=tf.nn.dropout(fc1,keep_prob)\n",
    "    fc2=fully_conn(fc1,256)\n",
    "    fc2=tf.nn.dropout(fc2,keep_prob)\n",
    "    fc3=fully_conn(fc2,128)\n",
    "    fc3=tf.nn.dropout(fc3,keep_prob)\n",
    "    fc4=fully_conn(fc3,128)\n",
    "    fc4=tf.nn.dropout(fc4,keep_prob)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    logits=output(fc4,10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run([optimizer],feed_dict={x: feature_batch,\n",
    "                                       y: label_batch,\n",
    "                                       keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss,acc=session.run([cost,accuracy],feed_dict={x: feature_batch,\n",
    "                                                y: label_batch,\n",
    "                                                keep_prob:1.0})\n",
    "    print(\"loss=\",loss,\"acc=\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss= 2.30309 acc= 0.15\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss= 2.30295 acc= 0.075\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss= 2.30305 acc= 0.075\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss= 2.3028 acc= 0.075\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss= 2.30277 acc= 0.125\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss= 2.30249 acc= 0.125\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss= 2.30161 acc= 0.125\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss= 2.29523 acc= 0.175\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss= 2.24403 acc= 0.225\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss= 2.19958 acc= 0.2\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss= 2.16087 acc= 0.2\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss= 2.07997 acc= 0.2\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss= 2.07989 acc= 0.15\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss= 2.06757 acc= 0.225\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss= 2.03794 acc= 0.275\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss= 2.08362 acc= 0.175\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss= 1.94772 acc= 0.3\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss= 1.89585 acc= 0.275\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss= 1.90419 acc= 0.275\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss= 2.02183 acc= 0.15\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss= 2.30106 acc= 0.15\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss= 2.30182 acc= 0.1\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss= 2.29973 acc= 0.15\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss= 2.29846 acc= 0.125\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss= 2.26461 acc= 0.175\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss= 2.28505 acc= 0.125\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss= 2.24917 acc= 0.075\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss= 2.07231 acc= 0.125\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss= 2.07164 acc= 0.2\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss= 2.15434 acc= 0.175\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss= 2.10141 acc= 0.175\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss= 1.9928 acc= 0.25\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss= 1.80149 acc= 0.25\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss= 1.92739 acc= 0.25\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss= 1.90962 acc= 0.2\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss= 2.02785 acc= 0.2\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss= 1.92408 acc= 0.375\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss= 1.72836 acc= 0.3\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss= 1.90293 acc= 0.25\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss= 1.86075 acc= 0.225\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss= 1.96126 acc= 0.275\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss= 1.87144 acc= 0.325\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss= 1.62597 acc= 0.45\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss= 1.73567 acc= 0.225\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss= 1.76899 acc= 0.3\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss= 1.85069 acc= 0.3\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss= 1.88299 acc= 0.275\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss= 1.53232 acc= 0.375\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss= 1.68716 acc= 0.25\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss= 1.61362 acc= 0.325\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss= 1.68781 acc= 0.375\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss= 1.80939 acc= 0.325\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss= 1.50823 acc= 0.425\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss= 1.68031 acc= 0.275\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss= 1.55153 acc= 0.325\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss= 1.56295 acc= 0.4\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss= 1.72543 acc= 0.4\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss= 1.51805 acc= 0.325\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss= 1.57488 acc= 0.3\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss= 1.6446 acc= 0.325\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss= 1.50326 acc= 0.425\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss= 1.73807 acc= 0.475\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss= 1.31777 acc= 0.525\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss= 1.56225 acc= 0.35\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss= 1.57765 acc= 0.375\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss= 1.37558 acc= 0.5\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss= 1.66226 acc= 0.4\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss= 1.35642 acc= 0.475\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss= 1.46858 acc= 0.4\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss= 1.53088 acc= 0.475\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss= 1.4607 acc= 0.425\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss= 1.40899 acc= 0.475\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss= 1.48482 acc= 0.425\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss= 1.42809 acc= 0.475\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss= 1.43099 acc= 0.425\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss= 1.27919 acc= 0.5\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss= 1.40978 acc= 0.475\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss= 1.28083 acc= 0.5\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss= 1.36848 acc= 0.55\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss= 1.42254 acc= 0.575\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss= 1.12906 acc= 0.55\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss= 1.29679 acc= 0.575\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss= 1.16726 acc= 0.625\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss= 1.29112 acc= 0.6\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss= 1.27432 acc= 0.55\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss= 1.09641 acc= 0.55\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss= 1.42495 acc= 0.575\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss= 1.21541 acc= 0.55\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss= 1.37437 acc= 0.5\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss= 1.36733 acc= 0.6\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss= 1.07181 acc= 0.575\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss= 1.18869 acc= 0.55\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss= 1.13762 acc= 0.65\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss= 1.2088 acc= 0.6\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss= 1.42845 acc= 0.575\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss= 1.03886 acc= 0.575\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss= 1.19829 acc= 0.6\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss= 1.0715 acc= 0.65\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss= 1.3704 acc= 0.525\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss= 1.47533 acc= 0.525\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss= 1.08231 acc= 0.575\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss= 1.12917 acc= 0.55\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss= 1.08792 acc= 0.6\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss= 1.20319 acc= 0.575\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss= 1.30242 acc= 0.575\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss= 0.903156 acc= 0.7\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss= 1.11565 acc= 0.6\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss= 1.02564 acc= 0.675\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss= 1.21042 acc= 0.65\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss= 1.17957 acc= 0.625\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss= 0.927119 acc= 0.6\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss= 1.12271 acc= 0.625\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss= 0.998741 acc= 0.7\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss= 1.17484 acc= 0.675\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss= 1.20423 acc= 0.625\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss= 0.895123 acc= 0.6\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss= 1.21429 acc= 0.675\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss= 1.01922 acc= 0.675\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss= 1.21043 acc= 0.575\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss= 1.03617 acc= 0.65\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss= 0.851951 acc= 0.65\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss= 1.01419 acc= 0.65\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss= 0.85618 acc= 0.775\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss= 0.925194 acc= 0.725\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss= 1.0959 acc= 0.65\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss= 0.840762 acc= 0.625\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss= 1.02547 acc= 0.625\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss= 0.844374 acc= 0.725\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss= 0.926625 acc= 0.7\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss= 1.12216 acc= 0.625\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss= 0.81423 acc= 0.675\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss= 1.04728 acc= 0.625\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss= 0.814712 acc= 0.825\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss= 1.0226 acc= 0.725\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss= 1.00863 acc= 0.725\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss= 0.752711 acc= 0.7\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss= 0.932237 acc= 0.7\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss= 0.748903 acc= 0.8\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss= 0.966084 acc= 0.65\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss= 1.02915 acc= 0.675\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss= 0.755122 acc= 0.75\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss= 0.852744 acc= 0.7\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss= 0.756432 acc= 0.825\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss= 0.88148 acc= 0.75\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss= 0.929826 acc= 0.775\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss= 0.673641 acc= 0.75\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss= 0.828793 acc= 0.7\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss= 0.713549 acc= 0.8\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss= 0.925973 acc= 0.75\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss= 0.879514 acc= 0.75\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss= 0.686709 acc= 0.825\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss= 0.786024 acc= 0.8\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss= 0.628189 acc= 0.825\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss= 0.770866 acc= 0.75\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss= 0.776596 acc= 0.725\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss= 0.612441 acc= 0.775\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss= 0.770304 acc= 0.775\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss= 0.649565 acc= 0.85\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss= 0.731785 acc= 0.75\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss= 0.835272 acc= 0.7\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss= 0.685942 acc= 0.775\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss= 0.652881 acc= 0.8\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss= 0.628564 acc= 0.825\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss= 0.675212 acc= 0.825\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss= 0.834798 acc= 0.725\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss= 0.615497 acc= 0.775\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss= 0.75817 acc= 0.775\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss= 0.567139 acc= 0.875\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss= 0.771708 acc= 0.775\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss= 0.825419 acc= 0.775\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss= 0.672965 acc= 0.75\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss= 0.725027 acc= 0.75\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss= 0.586372 acc= 0.8\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss= 0.665032 acc= 0.8\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss= 0.656606 acc= 0.8\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss= 0.626993 acc= 0.8\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss= 0.744627 acc= 0.65\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss= 0.577135 acc= 0.85\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss= 0.58006 acc= 0.825\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss= 0.665574 acc= 0.8\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss= 0.564616 acc= 0.825\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss= 0.634626 acc= 0.775\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss= 0.689512 acc= 0.8\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss= 0.680608 acc= 0.8\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss= 0.704159 acc= 0.8\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss= 0.579506 acc= 0.825\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss= 0.611891 acc= 0.825\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss= 0.571193 acc= 0.8\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss= 0.722075 acc= 0.75\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss= 0.731754 acc= 0.725\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss= 0.617416 acc= 0.775\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss= 0.583675 acc= 0.85\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss= 0.545244 acc= 0.875\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss= 0.586478 acc= 0.775\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss= 0.767522 acc= 0.7\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss= 0.557615 acc= 0.85\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss= 0.699577 acc= 0.725\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss= 0.479069 acc= 0.8\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss= 0.551016 acc= 0.8\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss= 0.516449 acc= 0.85\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss= 0.531544 acc= 0.825\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss= 0.597481 acc= 0.8\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss= 0.532135 acc= 0.825\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss= 0.452943 acc= 0.875\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss= 0.577147 acc= 0.8\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss= 0.58919 acc= 0.8\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss= 0.55313 acc= 0.775\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss= 0.451873 acc= 0.9\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss= 0.507432 acc= 0.85\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss= 0.654526 acc= 0.725\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss= 0.585238 acc= 0.825\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss= 0.551237 acc= 0.8\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss= 0.446191 acc= 0.9\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss= 0.536779 acc= 0.9\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss= 0.435488 acc= 0.9\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss= 0.499912 acc= 0.825\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss= 0.532766 acc= 0.825\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss= 0.427446 acc= 0.875\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss= 0.450699 acc= 0.9\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss= 0.456163 acc= 0.85\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss= 0.500924 acc= 0.8\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss= 0.620993 acc= 0.775\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss= 0.466672 acc= 0.825\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss= 0.436662 acc= 0.85\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss= 0.476201 acc= 0.8\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss= 0.435084 acc= 0.85\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss= 0.629432 acc= 0.775\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss= 0.441075 acc= 0.875\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss= 0.392175 acc= 0.9\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss= 0.396303 acc= 0.925\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss= 0.447353 acc= 0.825\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss= 0.521776 acc= 0.775\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss= 0.428386 acc= 0.875\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss= 0.383673 acc= 0.9\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss= 0.41065 acc= 0.875\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss= 0.404296 acc= 0.85\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss= 0.513297 acc= 0.75\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss= 0.461521 acc= 0.9\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss= 0.479537 acc= 0.875\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss= 0.434084 acc= 0.875\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss= 0.457561 acc= 0.8\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss= 0.511106 acc= 0.825\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss= 0.405937 acc= 0.875\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss= 0.422104 acc= 0.875\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss= 0.350739 acc= 0.95\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss= 0.417601 acc= 0.875\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss= 0.456067 acc= 0.85\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss= 0.365658 acc= 0.925\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss= 0.363025 acc= 0.9\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss= 0.305321 acc= 0.975\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss= 0.454844 acc= 0.875\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss= 0.467099 acc= 0.85\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss= 0.360658 acc= 0.875\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss= 0.358163 acc= 0.9\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss= 0.361036 acc= 0.975\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss= 0.421393 acc= 0.85\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss= 0.411075 acc= 0.85\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss= 0.536672 acc= 0.8\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss= 0.514175 acc= 0.9\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss= 0.347882 acc= 0.975\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss= 0.458111 acc= 0.825\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss= 0.524252 acc= 0.825\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss= 0.355566 acc= 0.9\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss= 0.322645 acc= 0.925\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss= 0.368899 acc= 0.9\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss= 0.396254 acc= 0.85\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss= 0.498586 acc= 0.825\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss= 0.350907 acc= 0.925\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss= 0.305039 acc= 0.9\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss= 0.26883 acc= 1.0\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss= 0.385139 acc= 0.85\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss= 0.422632 acc= 0.85\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss= 0.298192 acc= 0.95\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss= 0.339915 acc= 0.9\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss= 0.319302 acc= 0.975\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss= 0.375249 acc= 0.875\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss= 0.439432 acc= 0.825\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss= 0.281636 acc= 0.95\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss= 0.296072 acc= 0.9\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss= 0.251239 acc= 1.0\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss= 0.507144 acc= 0.825\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss= 0.385467 acc= 0.825\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss= 0.40625 acc= 0.875\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss= 0.333837 acc= 0.9\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss= 0.280067 acc= 0.95\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss= 0.334675 acc= 0.875\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss= 0.376095 acc= 0.825\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss= 0.294616 acc= 0.95\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss= 0.27136 acc= 0.925\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss= 0.230949 acc= 0.975\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss= 0.395939 acc= 0.875\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss= 0.42938 acc= 0.825\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss= 0.403766 acc= 0.9\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss= 0.279927 acc= 0.95\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss= 0.256984 acc= 1.0\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss= 0.334939 acc= 0.875\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss= 0.405464 acc= 0.85\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss= 0.26798 acc= 0.95\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss= 0.309389 acc= 0.9\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss= 0.256078 acc= 0.95\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss= 0.368171 acc= 0.875\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss= 0.371955 acc= 0.85\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss= 0.407792 acc= 0.85\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss= 0.263425 acc= 0.925\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss= 0.207277 acc= 1.0\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss= 0.393671 acc= 0.85\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss= 0.458677 acc= 0.85\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss= 0.297846 acc= 0.925\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss= 0.273674 acc= 0.95\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss= 0.212313 acc= 1.0\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss= 0.443554 acc= 0.825\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss= 0.381413 acc= 0.9\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss= 0.292969 acc= 0.95\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss= 0.205558 acc= 0.975\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss= 0.210158 acc= 0.975\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss= 0.422273 acc= 0.825\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss= 0.362668 acc= 0.925\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss= 0.232848 acc= 0.95\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss= 0.308832 acc= 0.9\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss= 0.225316 acc= 1.0\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss= 0.338732 acc= 0.9\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss= 0.324972 acc= 0.9\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss= 0.254545 acc= 0.95\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss= 0.211048 acc= 0.95\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss= 0.208789 acc= 1.0\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss= 0.34986 acc= 0.9\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss= 0.353967 acc= 0.9\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss= 0.243162 acc= 0.95\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss= 0.290758 acc= 0.9\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss= 0.207714 acc= 1.0\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss= 0.332749 acc= 0.9\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss= 0.322523 acc= 0.925\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss= 0.231211 acc= 0.95\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss= 0.30839 acc= 0.9\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss= 0.194322 acc= 1.0\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss= 0.357942 acc= 0.9\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss= 0.370219 acc= 0.875\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss= 0.222861 acc= 0.975\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss= 0.216274 acc= 0.95\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss= 0.196507 acc= 1.0\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss= 0.33482 acc= 0.9\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss= 0.337487 acc= 0.9\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss= 0.206694 acc= 0.95\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss= 0.207909 acc= 0.95\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss= 0.216053 acc= 1.0\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss= 0.381826 acc= 0.875\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss= 0.282739 acc= 0.95\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss= 0.253612 acc= 0.95\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss= 0.202753 acc= 0.925\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss= 0.154837 acc= 1.0\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss= 0.284362 acc= 0.925\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss= 0.328852 acc= 0.95\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss= 0.226773 acc= 0.95\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss= 0.166003 acc= 0.95\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss= 0.184266 acc= 1.0\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss= 0.340471 acc= 0.925\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss= 0.310982 acc= 0.925\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss= 0.204034 acc= 0.95\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss= 0.164575 acc= 0.975\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss= 0.157129 acc= 1.0\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss= 0.277958 acc= 0.95\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss= 0.394291 acc= 0.875\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss= 0.164653 acc= 0.975\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss= 0.214104 acc= 0.95\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss= 0.188057 acc= 0.95\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss= 0.258224 acc= 0.925\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss= 0.313842 acc= 0.95\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss= 0.18631 acc= 0.95\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss= 0.164254 acc= 0.975\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss= 0.155888 acc= 1.0\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss= 0.286595 acc= 0.9\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss= 0.295006 acc= 0.95\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss= 0.236178 acc= 0.925\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss= 0.206556 acc= 0.925\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss= 0.200518 acc= 0.975\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss= 0.277437 acc= 0.9\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss= 0.277707 acc= 0.925\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss= 0.149871 acc= 1.0\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss= 0.182551 acc= 0.975\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss= 0.184081 acc= 0.975\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss= 0.33339 acc= 0.875\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss= 0.294199 acc= 0.925\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss= 0.183547 acc= 0.975\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss= 0.194481 acc= 0.95\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss= 0.155844 acc= 0.975\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss= 0.298877 acc= 0.925\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss= 0.35502 acc= 0.95\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss= 0.14836 acc= 0.975\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss= 0.15034 acc= 1.0\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss= 0.224303 acc= 0.95\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss= 0.322035 acc= 0.9\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss= 0.286497 acc= 0.95\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss= 0.246376 acc= 0.925\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss= 0.121493 acc= 1.0\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss= 0.144087 acc= 0.975\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss= 0.263767 acc= 0.925\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss= 0.288513 acc= 0.95\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss= 0.108907 acc= 1.0\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss= 0.136597 acc= 1.0\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss= 0.127355 acc= 1.0\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss= 0.268524 acc= 0.925\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss= 0.253076 acc= 0.975\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss= 0.135434 acc= 1.0\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss= 0.137852 acc= 1.0\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss= 0.129653 acc= 1.0\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss= 0.261988 acc= 0.95\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss= 0.252495 acc= 0.975\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss= 0.145384 acc= 1.0\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss= 0.171446 acc= 0.95\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss= 0.154907 acc= 1.0\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss= 0.238587 acc= 0.925\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss= 0.243636 acc= 0.975\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss= 0.111992 acc= 1.0\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss= 0.116887 acc= 1.0\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss= 0.110319 acc= 1.0\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss= 0.207304 acc= 0.925\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss= 0.277653 acc= 0.95\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss= 0.116636 acc= 1.0\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss= 0.114028 acc= 0.975\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss= 0.104863 acc= 1.0\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss= 0.276393 acc= 0.95\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss= 0.216502 acc= 0.975\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss= 0.0980511 acc= 1.0\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss= 0.191892 acc= 0.95\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss= 0.138281 acc= 1.0\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss= 0.230876 acc= 0.925\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss= 0.312091 acc= 0.925\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss= 0.191299 acc= 0.975\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss= 0.166776 acc= 0.975\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss= 0.104627 acc= 1.0\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss= 0.271428 acc= 0.95\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss= 0.28634 acc= 0.9\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss= 0.109468 acc= 0.975\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss= 0.118396 acc= 1.0\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss= 0.116708 acc= 1.0\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss= 0.237697 acc= 0.95\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss= 0.19242 acc= 0.95\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss= 0.113221 acc= 1.0\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss= 0.13954 acc= 0.975\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss= 0.117243 acc= 1.0\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss= 0.209215 acc= 0.95\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss= 0.25227 acc= 0.95\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss= 0.111828 acc= 0.975\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss= 0.139106 acc= 0.975\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss= 0.136062 acc= 1.0\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss= 0.224917 acc= 0.95\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss= 0.263053 acc= 0.95\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss= 0.117481 acc= 1.0\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss= 0.0910033 acc= 1.0\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss= 0.12408 acc= 1.0\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss= 0.21456 acc= 0.95\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss= 0.200872 acc= 0.975\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss= 0.098862 acc= 1.0\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss= 0.155646 acc= 0.975\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss= 0.103708 acc= 1.0\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss= 0.178693 acc= 0.95\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss= 0.235291 acc= 0.975\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss= 0.0830512 acc= 1.0\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss= 0.0942602 acc= 1.0\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss= 0.0978414 acc= 1.0\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss= 0.245195 acc= 0.95\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss= 0.225991 acc= 0.975\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss= 0.115549 acc= 1.0\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss= 0.118933 acc= 0.975\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss= 0.100633 acc= 1.0\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss= 0.170905 acc= 0.975\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss= 0.270792 acc= 0.9\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss= 0.124787 acc= 1.0\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss= 0.107346 acc= 1.0\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss= 0.0722425 acc= 1.0\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss= 0.193974 acc= 0.95\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss= 0.175031 acc= 1.0\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss= 0.111927 acc= 1.0\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss= 0.0958914 acc= 1.0\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss= 0.117492 acc= 0.975\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss= 0.205005 acc= 0.95\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss= 0.188216 acc= 0.975\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss= 0.122709 acc= 1.0\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss= 0.0774573 acc= 1.0\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss= 0.0859906 acc= 1.0\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss= 0.206671 acc= 0.95\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss= 0.141839 acc= 1.0\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss= 0.127043 acc= 1.0\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss= 0.0863442 acc= 1.0\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss= 0.0783016 acc= 1.0\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss= 0.240458 acc= 0.95\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss= 0.156668 acc= 0.975\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss= 0.0734577 acc= 1.0\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss= 0.16588 acc= 0.975\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss= 0.103707 acc= 1.0\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss= 0.162558 acc= 0.95\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss= 0.200163 acc= 0.975\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss= 0.14007 acc= 1.0\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss= 0.0823153 acc= 1.0\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss= 0.0812433 acc= 1.0\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss= 0.161086 acc= 0.95\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss= 0.203462 acc= 0.975\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss= 0.115452 acc= 1.0\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss= 0.0799852 acc= 1.0\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss= 0.0864542 acc= 1.0\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss= 0.21105 acc= 0.95\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss= 0.190941 acc= 0.975\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss= 0.0917578 acc= 1.0\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss= 0.0712303 acc= 1.0\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss= 0.0836019 acc= 1.0\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss= 0.218645 acc= 0.95\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss= 0.178708 acc= 1.0\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss= 0.0885095 acc= 1.0\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss= 0.0999447 acc= 0.975\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss= 0.14966 acc= 1.0\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss= 0.220672 acc= 0.95\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss= 0.171447 acc= 0.95\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss= 0.110092 acc= 1.0\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss= 0.0776222 acc= 1.0\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss= 0.0839767 acc= 1.0\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss= 0.190151 acc= 0.95\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss= 0.203575 acc= 0.95\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss= 0.0946175 acc= 1.0\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss= 0.0822402 acc= 0.975\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss= 0.119448 acc= 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.75029296875\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAJ/CAYAAAB/WMU1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIdDRNiEVERESko2lCLCIiIiId7f8DA+l8egz22UYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f205e629d30>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 354
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
